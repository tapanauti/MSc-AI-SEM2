{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## CT4101 - Deep Learning CT5133\n",
    "### Student Name(s): Tapan Auti, Atharva Kulkarni\n",
    "### Student ID(s): 20231499, 20231773"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Part 1\n",
    "\n",
    "Neural Network Approach to Logistic Regression:\n",
    "\n",
    "Logistic Regression is a Classification method where the goal of the algorithm is to find a dividing line between classes. This is contrary to Linear Regression where the goal of the algorithm is to find a line that goes through all the data points.\n",
    "\n",
    "Using a Neural Network Approach to Logistic Regression would be as follows:\n",
    "\n",
    "Logistic Regression is equivalent to a one layer deep neural network\n",
    "\n",
    "1. We have a single neuron that recieves an input with a weight and a bias.\n",
    "2. This neuron has an activation function (suppose a sigmoid/logit function) which activates when the weighted sum is above a paricular threshold\n",
    "\n",
    "Description of Algorithm:\n",
    "\n",
    "\n",
    "\n",
    "Blobs Dataset - Linearly Separable\n",
    "\n",
    "Moons Dataset - Non Separable\n",
    "\n",
    "References:\n",
    "\n",
    "1. Prof M.Madden - Lecture Notes, CT5133 - Deep Learning\n",
    "2. [Towards Data Science - Intro To Logistic Regression](https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148#:~:text=Logistic%20Regression%20is%20a%20Machine,on%20the%20concept%20of%20probability.&text=The%20hypothesis%20of%20logistic%20regression,function%20between%200%20and%201%20.)\n",
    "3. [Logistic Regression Using a Neural Network Mindset](https://www.youtube.com/watch?v=rGoJAL-rkE0&ab_channel=TechwithKumar)\n",
    "4. [But What is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk&ab_channel=3Blue1Brown)\n",
    "5. [Logistic Regression](https://www.youtube.com/watch?v=zM4VZR0px8E&ab_channel=codebasics)\n",
    "6. [Deep Learning, Logistic Regression with a Neural Network Mindset](https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Logistic%20Regression%20with%20a%20Neural%20Network%20mindset.ipynb)\n",
    "7. [Dummy Variable and One Hot Encoding](https://www.youtube.com/watch?v=9yl6-HEY7_s&ab_channel=codebasics)\n",
    "8. [Logistic Regression using a Neural Network Approach](https://datascience-enthusiast.com/DL/Logistic-Regression-with-a-Neural-Network-mindset.html)\n",
    "9. [Log Regression NN Mindset](https://edorado93.github.io/2018/09/07/Logistic-Regression-with-a-Neural-Networks-Mindset-9b5526c2ed46/)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display plots inline and change default figure size\n",
    "%matplotlib inline\n",
    "\n",
    "# 1 Author: Atharva\n",
    "# Sigmoid Function Definition\n",
    "\n",
    "\n",
    "def f(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "# 1 Author: Tapan\n",
    "# Calculate loss function\n",
    "def loss_gradient(w, x, b):\n",
    "    z = np.dot(x, w)\n",
    "    y = f(z)\n",
    "    loss = np.mean(-b * np.log(y) - (1 - b) * np.log(1 - y))\n",
    "    gdn = np.dot(x.T, (b - y)) / b.size\n",
    "    # print(b.shape)\n",
    "    # print(y.shape)\n",
    "\n",
    "    # 1 Author: Atharva\n",
    "    # Normalising Gradient Descent\n",
    "    grad = gdn + 300 / (2 * b.size) * np.concatenate(([0], w[1:])).T\n",
    "    return loss, grad\n",
    "\n",
    "\n",
    "# 1 Author: Tapan\n",
    "# Training Function\n",
    "def model_fit(x, b, total_iterations=5000):\n",
    "    final_w = []\n",
    "    label = np.unique(b)\n",
    "    all_loss = np.zeros(total_iterations)\n",
    "\n",
    "    for c in label:\n",
    "        b = np.where(b == c, 1, 0)\n",
    "        w = np.zeros(x.shape[1])\n",
    "        for i in range(total_iterations):\n",
    "            all_loss[i], grad = loss_gradient(w, x, b)\n",
    "            w += 0.01 * grad\n",
    "        final_w.append(w)\n",
    "    return final_w, label, all_loss\n",
    "\n",
    "\n",
    "# 1 Author: Atharva\n",
    "# Predict Function\n",
    "def predict_prob(label, w, x):\n",
    "    preds = [np.argmax([f(i@j) for j in final_w]) for i in x]\n",
    "    return [label[p] for p in preds]\n",
    "\n",
    "\n",
    "# 1 Author: Atharva\n",
    "# Calculate Accuracb of the model\n",
    "def accuracy(label, w, x, b):\n",
    "    acc = predict_prob(label, w, x)\n",
    "    return (acc == b).mean()\n"
   ]
  },
  {
   "source": [
    "# Part 2\n",
    "\n",
    "References:\n",
    "\n",
    "1. But What is A"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'moons400.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e4a807c74b69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"moons400.csv\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Moons Dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"blobs250.csv\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Blobs Dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# y = df['Class'].values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'moons400.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"moons400.csv\") # Moons Dataset\n",
    "df2 = pd.read_csv(\"blobs250.csv\") # Blobs Dataset\n",
    "\n",
    "\n",
    "# y = df['Class'].values\n",
    "# y2 = df2['Class'].values\n",
    "\n",
    "# # Drop the 'Class' Columns\n",
    "# del df['Class']\n",
    "# del df2['Class']\n",
    "\n",
    "# X = df.values   \n",
    "# X2 = df2.values\n",
    "\n",
    "# Test the model on Moons Dataset\n",
    "data_moons = np.array(df)\n",
    "\n",
    "num_train = int(.70 * len(data_moons))\n",
    "num_test = int(0.15 * len(data_moons))\n",
    "\n",
    "x_train, y_train = data_moons[:num_train, :-1], data_moons[:num_train, -1]\n",
    "x_test, y_test = data_moons[num_test:, :-1], data_moons[num_test:, -1]\n",
    "\n",
    "final_w, label, losses = model_fit(x_train, y_train)\n",
    "print(f\"Test Accuracy Moons  : {100*accuracy(label, final_w, x_test, y_test):.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Accuracy Blobs  : 49.30%\n"
     ]
    }
   ],
   "source": [
    "# Test the model on Blobs Dataset\n",
    "data_blobs = np.array(df2)\n",
    "\n",
    "num_train = int(.70 * len(data_blobs))\n",
    "num_test = int(0.15 * len(data_blobs))\n",
    "\n",
    "x_train, y_train = data_blobs[:num_train, :-1], data_blobs[:num_train, -1]\n",
    "x_test, y_test = data_blobs[num_test:, :-1], data_blobs[num_test:, -1]\n",
    "\n",
    "final_w, label, losses = model_fit(x_train, y_train)\n",
    "print(f\"Test Accuracy Blobs  : {100*accuracy(label, final_w, x_test, y_test):.2f}%\")"
   ]
  },
  {
   "source": [
    "### Observations\n",
    "\n",
    "1. The Moons dataset provides us an accuracy of 51.47%\n",
    "2. The Blobs dataset providds us an accuracy of 49.3%\n",
    "3. Even after being linearly separable, the Blobs dataset is predicted with an accuracy lower than that of the Moons dataset, when using Logistic Regression."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Part 3\n",
    "\n",
    "Description of Shallow Neural Network Algorithm:\n",
    "\n",
    "\n",
    "\n",
    "References\n",
    "1. [Shallow Neural Networks Explained](https://towardsdatascience.com/shallow-neural-networks-23594aa97a5#:~:text=Shallow%20neural%20networks%20consist%20of,inside%20a%20deep%20neural%20network.&text=The%20figure%20below%20shows%20a,layer%20and%201%20output%20layer.)\n",
    "2. [Building](https://towardsdatascience.com/building-a-shallow-neural-network-a4e2728441e0)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \n",
    "    np.random.seed(2) \n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros(shape=(n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros(shape=(n_y, 1))\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def relu(x):\n",
    "    return x*(x>0)\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def tanh(x):\n",
    "    #return np.exp((np.exp(x)-np.exp(-x))-(np.exp(x)+np.exp(-x)))\n",
    "    #return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "    return np.tanh(x)\n",
    "    #return math.tanh(x)\n",
    "\n",
    "def forward_pass(X, parameters):\n",
    "    \n",
    "    # to make forward pass calculations we need W1 and W2 so we will extract them from dictionary parameters\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    b1 = parameters['b1']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    # first layer calculations - hidden layer calculations\n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = tanh(Z1)  # activation in the first layer is tanh\n",
    "    \n",
    "    # output layer calculations\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)# A2 are predictions, y_hat\n",
    "    \n",
    "    # cache values for backpropagation calculations\n",
    "    cache = {'Z1':Z1,\n",
    "             'A1':A1,\n",
    "             'Z2':Z2,\n",
    "             'A2':A2\n",
    "            }\n",
    "    # print(A2.shape)\n",
    "    return A2, cache\n",
    "\n",
    "def backward_pass(parameters, cache, X, Y):\n",
    "    \n",
    "    # unpack paramaeters and cache to get values for backpropagation calculations\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    Z1 = cache['Z1']\n",
    "    A1 = cache['A1']\n",
    "    Z2 = cache['Z2']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    m = X.shape[1] # number of examples in a training set\n",
    "    #print(A2)\n",
    "    #print(A2.shape)\n",
    "    dZ2= A2 - Y\n",
    "    \n",
    "    \n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)   # keepdims - prevents python to output rank 1 array (n,)\n",
    "    \n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2)) # we use tanh activation function\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)  \n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, learning_rate, grads):\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    # updated parameters\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def NN_model(X,Y,n_h, num_iterations, learning_rate):\n",
    "    \n",
    "    n_x = X.shape[0] # size of an input layer = number of features \n",
    "    n_y = Y.shape[0] # size of an output layer\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    #unpack parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        A2, cache = forward_pass(X, parameters)\n",
    "        grads = backward_pass(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, learning_rate, grads)\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def predict(parameters, X):\n",
    "    \n",
    "    A2, cache = forward_pass(X, parameters)\n",
    "    predictions = np.round(A2)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_moons = np.array(df)\n",
    "\n",
    "num_train = int(.70 * len(data_moons))\n",
    "num_test = int(0.15 * len(data_moons))\n",
    "\n",
    "x_train, y_train = data_moons[:num_train, :-1], data_moons[:num_train, -1]\n",
    "x_test, y_test = data_moons[num_test:, :-1], data_moons[num_test:, -1]\n",
    "\n",
    "num_iterations = 20000\n",
    "learning_rate = 0.1\n",
    "n_h = 4\n",
    "parameters_final = NN_model(x_train.T,y_train,n_h, num_iterations, learning_rate)\n",
    "\n",
    "Y_predictions_test = predict(parameters_final, x_test.T)\n",
    "Y_predictions_train = predict(parameters_final, x_train.T)\n",
    "\n",
    "\n",
    "print(f\"Train accuracy Moons: {np.mean(y_train == Y_predictions_train)}\")\n",
    "print(f\"Test accuracy Moons: {np.mean(y_test == Y_predictions_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_blobs = np.array(df2)\n",
    "\n",
    "num_train = int(.70 * len(data_blobs))\n",
    "num_test = int(0.15 * len(data_blobs))\n",
    "\n",
    "x_train, y_train = data_blobs[:num_train, :-1], data_blobs[:num_train, -1]\n",
    "x_test, y_test = data_blobs[num_test:, :-1], data_blobs[num_test:, -1]\n",
    "\n",
    "num_iterations = 20000\n",
    "learning_rate = 0.1\n",
    "n_h = 4\n",
    "parameters_final = NN_model(x_train.T,y_train,n_h, num_iterations, learning_rate)\n",
    "\n",
    "Y_predictions_test = predict(parameters_final, x_test.T)\n",
    "Y_predictions_train = predict(parameters_final, x_train.T)\n",
    "\n",
    "\n",
    "print(f\"Train accuracy Blobs: {np.mean(y_train == Y_predictions_train)}\")\n",
    "print(f\"Test accuracy Blobs: {np.mean(y_test == Y_predictions_test)}\")"
   ]
  },
  {
   "source": [
    "# Part 4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of items in the batch is 4\nAll keys in the batch: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\nsize of data in this batch: 10000 , size of labels: 10000\n<class 'numpy.ndarray'>\n(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "# This function taken from the CIFAR website\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Loaded in this way, each of the batch files contains a dictionary with the following elements:\n",
    "#   data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. \n",
    "#           The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. \n",
    "#           The image is stored in row-major order, so that the first 32 entries of the array are the red channel values \n",
    "#           of the first row of the image.\n",
    "#   labels -- a list of 10000 numbers in the range 0-9. \n",
    "#             The number at index i indicates the label of the ith image in the array data.\n",
    "\n",
    "def loadbatch(batchname):\n",
    "    folder = 'cifar-10-batches-py'\n",
    "    batch = unpickle(folder+\"/\"+batchname)\n",
    "    return batch\n",
    "\n",
    "def loadlabelnames():\n",
    "    folder = 'cifar-10-batches-py'\n",
    "    meta = unpickle(folder+\"/\"+'batches.meta')\n",
    "    return meta[b'label_names']\n",
    "\n",
    "batch1 = loadbatch('data_batch_1')\n",
    "print(\"Number of items in the batch is\", len(batch1))\n",
    "\n",
    "# Display all keys, so we can see the ones we want\n",
    "print('All keys in the batch:', batch1.keys())\n",
    "\n",
    "data_cifar = batch1[b'data']\n",
    "labels_cifar = batch1[b'labels']\n",
    "print (\"size of data in this batch:\", len(data_cifar), \", size of labels:\", len(labels_cifar))\n",
    "print (type(data_cifar))\n",
    "print(data_cifar.shape)\n",
    "\n",
    "names = loadlabelnames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[255 253 253 ...  83  83  84]\n [ 28  37  38 ...  28  37  46]\n [170 168 177 ...  82  78  80]\n ...\n [134 131 128 ... 136 137 138]\n [125 110 102 ...  82  84  86]\n [ 53  54  56 ...  39  46  41]]\n(99, 3072)\n(99, 1023)\n[[255 253 253 ...  68  78  79]\n [ 28  37  38 ...  67  54  63]\n [170 168 177 ...  71  75  71]\n [159 150 153 ... 157 166 173]]\n[[154 126 105 ... 134 140 143]\n [255 253 253 ...  58  68  78]\n [ 28  37  38 ...  84  67  54]\n ...\n [221 214 216 ... 143 140 138]\n [110 103 104 ...  75  91  98]\n [151 150 151 ...  99 101 100]]\n"
     ]
    }
   ],
   "source": [
    "# print(data)\n",
    "data_new = data_cifar[1:100]\n",
    "print(data_new[1:10])\n",
    "print(data_new.shape)\n",
    "\n",
    "data_new = data_new[:,:1023]\n",
    "\n",
    "print(data_new.shape)\n",
    "print(data_new[1:5])\n",
    "\n",
    "num_train = int(.70 * len(data_new))\n",
    "num_test = int(0.15 * len(data_new))\n",
    "\n",
    "x_train, y_train = data_new[:num_train, :-1], data_new[:num_train, -1]\n",
    "x_test, y_test = data_new[num_test:, :-1], data_new[num_test:, -1]\n",
    "\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train accuracy: 0.0\nTest accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 20000\n",
    "learning_rate = 0.1\n",
    "n_h = 4\n",
    "parameters_final = NN_model(x_train.T,y_train,n_h, num_iterations, learning_rate)\n",
    "\n",
    "Y_predictions_test = predict(parameters_final, x_test.T)\n",
    "Y_predictions_train = predict(parameters_final, x_train.T)\n",
    "\n",
    "#print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_predictions_train - y_train)) * 100))\n",
    "print(f\"Train accuracy: {np.mean(y_train == Y_predictions_train)}\")\n",
    "print(f\"Test accuracy: {np.mean(y_test == Y_predictions_test)}\")\n",
    "\n",
    "#print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_predictions_test - y_test)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def loadbatch(batchname):\n",
    "    batch = unpickle('cifar-10-batches-py'+\"/\"+batchname)\n",
    "    return batch\n",
    "\n",
    "def loadlabelnames():\n",
    "    meta = unpickle('cifar-10-batches-py'+\"/\"+'batches.meta')\n",
    "    return meta[b'label_names']\n",
    "\n",
    "batch1 = loadbatch('data_batch_1')\n",
    "print(\"Number of items in the batch is\", len(batch1))\n",
    "\n",
    "# Display all keys, so we can see the ones we want\n",
    "print('All keys in the batch:', batch1.keys())\n",
    "\n",
    "data = batch1[b'data'][:,:1023]\n",
    "labels = batch1[b'labels']\n",
    "print (\"size of data in this batch:\", len(data), \", size of labels:\", len(labels))\n",
    "print (type(data))\n",
    "# print(data.shape)\n",
    "# print(labels)\n",
    "label = []\n",
    "data_class = []\n",
    "names = loadlabelnames()\n",
    "\n",
    "for i in range(len(labels)):\n",
    "  if labels[i] == 1 or labels[i]==6:\n",
    "    label.append(labels[i])\n",
    "    data_class.append(data[i])\n",
    "\n",
    "df = np.array(data_class)\n",
    "df2 = np.array(label)\n",
    "\n",
    "\n",
    "num_train = int(.70 * len(df))\n",
    "num_test = int(0.15 * len(df))\n",
    "\n",
    "x_train, y_train = data_class[:num_train], label[:num_train]\n",
    "x_test, y_test = data_class[num_test:], label[num_test:]\n",
    "\n",
    "x_train  = np.array(x_train)\n",
    "y_train  = np.array(y_train)\n",
    "x_test  = np.array(x_test)\n",
    "y_test  = np.array(y_test)\n",
    "\n",
    "arr = [y_train.tolist()]\n",
    "y_train = np.array(arr)\n",
    "\n",
    "arr1 = [y_test.tolist()]\n",
    "y_test = np.array(arr1)\n",
    "\n",
    "num_iterations = 20000\n",
    "learning_rate = 0.01\n",
    "n_h = 4\n",
    "parameters_final = NN_model(x_train.T,y_train,n_h, num_iterations, learning_rate)\n",
    "\n",
    "Y_predictions_test = predict(parameters_final, x_test.T)\n",
    "Y_predictions_train = predict(parameters_final, x_train.T)\n",
    "\n",
    "\n",
    "acc = np.mean(y_train == Y_predictions_train)\n",
    "acc"
   ]
  },
  {
   "source": [
    "### Part 5\n",
    "\n",
    "### Tapan Auti\n",
    "\n",
    "### Resources:\n",
    "\n",
    "1. https://towardsdatascience.com/how-to-implement-an-adam-optimizer-from-scratch-76e7b217f1cc\n",
    "2. https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c\n",
    "3. https://machinelearningmastery.com/tour-of-optimization-algorithms/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros(shape=(n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros(shape=(n_y, 1))\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def forward_pass(X, parameters):\n",
    "    #print(parameters)\n",
    "    # to make forward pass calculations we need W1 and W2 so we will extract them from dictionary parameters\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    b1 = parameters['b1']\n",
    "    b2 = parameters['b2']\n",
    "    #print(parameters)\n",
    "    #print(W1)\n",
    "    #print(b1)\n",
    "    # first layer calculations - hidden layer calculations\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    #print(Z1)\n",
    "    # for i in range(len(Z1)):\n",
    "    #   print(i)\n",
    "    #   print(Z1[i])\n",
    "    A1 = tanh(Z1)  # activation in the first layer is tanh\n",
    "    #print(A1)\n",
    "    # output layer calculations\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)# A2 are predictions, y_hat\n",
    "    \n",
    "    # cache values for backpropagation calculations\n",
    "    cache = {'Z1':Z1,\n",
    "             'A1':A1,\n",
    "             'Z2':Z2,\n",
    "             'A2':A2\n",
    "            }\n",
    "    # print(A2.shape)\n",
    "    return A2, cache\n",
    "\n",
    "def backward_pass(parameters, cache, X, Y):\n",
    "    \n",
    "    # unpack paramaeters and cache to get values for backpropagation calculations\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    Z1 = cache['Z1']\n",
    "    A1 = cache['A1']\n",
    "    Z2 = cache['Z2']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    m = X.shape[1] # number of examples in a training set\n",
    "    #print(A2)\n",
    "    #print(A2.shape)\n",
    "    dZ2= A2 - Y\n",
    "    \n",
    "    \n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)   # keepdims - prevents python to output rank 1 array (n,)\n",
    "    \n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2)) # we use tanh activation function\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)  \n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def initialize_adam(parameters) :\n",
    "       \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # v- exponentially weighted average of the gradient\n",
    "    # s -exponentially weighted average of the squared gradient\n",
    "    for l in range(L):\n",
    "        v[\"dW1\" ] = np.zeros(parameters[\"W1\" ].shape)\n",
    "        v[\"db1\" ] = np.zeros(parameters[\"b1\" ].shape)\n",
    "        s[\"dW2\" ] = np.zeros(parameters[\"W2\" ].shape)\n",
    "        s[\"db2\" ] = np.zeros(parameters[\"b2\" ].shape)\n",
    "        \n",
    "    return v, s\n",
    "\n",
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "   \n",
    "  \n",
    "    \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "        \n",
    "    for l in range(L):\n",
    "        v[\"dW1\"] = beta1*v[\"dW1\" ] + (1-beta1)*grads['dW1' ] \n",
    "        v[\"db1\" ] = beta1*v[\"db1\" ] + (1-beta1)*grads['db1' ]\n",
    "       \n",
    "\n",
    "        # Compute bias-corrected first moment estimate. \n",
    "       \n",
    "        v_corrected[\"dW1\" ] = v[\"dW1\" ]/(1 - beta1**t)\n",
    "        v_corrected[\"db1\" ] = v[\"db1\" ]/(1 - beta1**t)\n",
    "\n",
    "\n",
    "        # Moving average of the squared gradients.\n",
    "     \n",
    "        s[\"dW2\"] = beta2*s[\"dW2\" ] + (1-beta2)*(grads['dW2' ])**2\n",
    "        s[\"db2\" ] = beta2*s[\"db2\" ] + (1-beta2)*(grads['db2' ])**2\n",
    "       \n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. \n",
    "       \n",
    "        s_corrected[\"dW2\" ] = s[\"dW2\" ]/(1 - beta2**t)\n",
    "        s_corrected[\"db2\" ] = s[\"db2\" ]/(1 - beta2**t)\n",
    "        \n",
    "\n",
    "        parameters[\"W1\" ] = parameters[\"W1\" ]- (learning_rate*v_corrected[\"dW1\" ])/(np.sqrt(s_corrected[\"dW2\" ].T+epsilon))\n",
    "        parameters[\"b1\" ] = parameters[\"b1\" ]- (learning_rate*v_corrected[\"db1\" ])/(np.sqrt(s_corrected[\"db2\" ].T+epsilon))\n",
    "   \n",
    "    return parameters, v, s\n",
    "\n",
    "def NN_model(X,Y,n_h, num_iterations, learning_rate):\n",
    "    \n",
    "    n_x = X.shape[0] # size of an input layer = number of features \n",
    "    n_y = Y.shape[0] # size of an output layer\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    #unpack parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    #print(parameters)\n",
    "    param = {}\n",
    "    for i in range(num_iterations):\n",
    "      #print(i)  \n",
    "      A2, cache = forward_pass(X, parameters)\n",
    "      grads = backward_pass(parameters, cache, X, Y)\n",
    "      v,s = initialize_adam(parameters)\n",
    "      param = update_parameters_with_adam(parameters,grads,v,s , t = 2 )\n",
    "      #print(i,param)\n",
    "    #print(A2)\n",
    "    # print(cache)    \n",
    "    return parameters\n",
    "\n",
    "def predict(parameters, X):\n",
    "    \n",
    "    A2, cache = forward_pass(X, parameters)\n",
    "    predictions = np.round(A2)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def loadbatch(batchname):\n",
    "    batch = unpickle('cifar-10-batches-py'+\"/\"+batchname)\n",
    "    return batch\n",
    "\n",
    "def loadlabelnames():\n",
    "    meta = unpickle('cifar-10-batches-py'+\"/\"+'batches.meta')\n",
    "    return meta[b'label_names']\n",
    "\n",
    "batch1 = loadbatch('data_batch_1')\n",
    "print(\"Number of items in the batch is\", len(batch1))\n",
    "\n",
    "# Display all keys, so we can see the ones we want\n",
    "print('All keys in the batch:', batch1.keys())\n",
    "\n",
    "data = batch1[b'data'][:,:1023]\n",
    "labels = batch1[b'labels']\n",
    "print (\"size of data in this batch:\", len(data), \", size of labels:\", len(labels))\n",
    "print (type(data))\n",
    "# print(data.shape)\n",
    "# print(labels)\n",
    "label = []\n",
    "data_class = []\n",
    "names = loadlabelnames()\n",
    "\n",
    "for i in range(len(labels)):\n",
    "  if labels[i] == 1 or labels[i]==6:\n",
    "    label.append(labels[i])\n",
    "    data_class.append(data[i])\n",
    "\n",
    "df = np.array(data_class)\n",
    "df2 = np.array(label)\n",
    "\n",
    "\n",
    "num_train = int(.70 * len(df))\n",
    "num_test = int(0.15 * len(df))\n",
    "\n",
    "x_train, y_train = data_class[:num_train], label[:num_train]\n",
    "x_test, y_test = data_class[num_test:], label[num_test:]\n",
    "\n",
    "x_train  = np.array(x_train)\n",
    "y_train  = np.array(y_train)\n",
    "x_test  = np.array(x_test)\n",
    "y_test  = np.array(y_test)\n",
    "\n",
    "arr = [y_train.tolist()]\n",
    "y_train = np.array(arr)\n",
    "\n",
    "arr1 = [y_test.tolist()]\n",
    "y_test = np.array(arr1)\n",
    "\n",
    "num_iterations = 20000\n",
    "learning_rate = 0.01\n",
    "n_h = 4\n",
    "parameters_final = NN_model(x_train.T,y_train,n_h, num_iterations, learning_rate)\n",
    "\n",
    "Y_predictions_test = predict(parameters_final, x_test.T)\n",
    "Y_predictions_train = predict(parameters_final, x_train.T)\n",
    "\n",
    "\n",
    "acc = np.mean(y_train == Y_predictions_train)\n",
    "acc"
   ]
  },
  {
   "source": [
    "# Part 5 : Atharva Kulkarni\n",
    "\n",
    "Changed the activation function from a standard sigmoid to ReLU:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_relu(X, parameters):\n",
    "    \n",
    "    # to make forward pass calculations we need W1 and W2 so we will extract them from dictionary parameters\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    b1 = parameters['b1']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    # first layer calculations - hidden layer calculations\n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = relu(Z1)  # activation in the first layer is tanh\n",
    "    \n",
    "    # output layer calculations\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)# A2 are predictions, y_hat\n",
    "    \n",
    "    # cache values for backpropagation calculations\n",
    "    cache = {'Z1':Z1,\n",
    "             'A1':A1,\n",
    "             'Z2':Z2,\n",
    "             'A2':A2\n",
    "            }\n",
    "    # print(A2.shape)\n",
    "    return A2, cache\n",
    "\n",
    "def NN_model_relu(X,Y,n_h, num_iterations, learning_rate):\n",
    "    \n",
    "    n_x = X.shape[0] # size of an input layer = number of features \n",
    "    n_y = Y.shape[0] # size of an output layer\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    #unpack parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        A2, cache = forward_pass_relu(X, parameters)\n",
    "        grads = backward_pass(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, learning_rate, grads)\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def predict_relu(parameters, X):\n",
    "    \n",
    "    A2, cache = forward_pass_relu(X, parameters)\n",
    "    predictions = np.round(A2)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000\n",
    "learning_rate = 0.01\n",
    "n_h = 4\n",
    "parameters_final = NN_model_relu(x_train.T,y_train,n_h, num_iterations, learning_rate)\n",
    "\n",
    "Y_predictions_test = predict_relu(parameters_final, x_test.T)\n",
    "Y_predictions_train = predict_relu(parameters_final, x_train.T)\n",
    "\n",
    "\n",
    "acc = np.mean(y_train == Y_predictions_train)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}