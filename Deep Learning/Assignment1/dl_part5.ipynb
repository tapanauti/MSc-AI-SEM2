{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl_part5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4xZ4deV88RX"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "sns.set()"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmz-_sUkFmn9"
      },
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \n",
        "    #\n",
        "    \n",
        "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
        "    b1 = np.zeros(shape=(n_h, 1))\n",
        "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
        "    b2 = np.zeros(shape=(n_y, 1))\n",
        "    \n",
        "    assert (W1.shape == (n_h, n_x))\n",
        "    assert (b1.shape == (n_h, 1))\n",
        "    assert (W2.shape == (n_y, n_h))\n",
        "    assert (b2.shape == (n_y, 1))\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpeJunOfGxxM"
      },
      "source": [
        "def forward_pass(X, parameters):\n",
        "    #print(parameters)\n",
        "    # to make forward pass calculations we need W1 and W2 so we will extract them from dictionary parameters\n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "    b1 = parameters['b1']\n",
        "    b2 = parameters['b2']\n",
        "    #print(parameters)\n",
        "    #print(W1)\n",
        "    #print(b1)\n",
        "    # first layer calculations - hidden layer calculations\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    #print(Z1)\n",
        "    # for i in range(len(Z1)):\n",
        "    #   print(i)\n",
        "    #   print(Z1[i])\n",
        "    A1 = tanh(Z1)  # activation in the first layer is tanh\n",
        "    #print(A1)\n",
        "    # output layer calculations\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2)# A2 are predictions, y_hat\n",
        "    \n",
        "    # cache values for backpropagation calculations\n",
        "    cache = {'Z1':Z1,\n",
        "             'A1':A1,\n",
        "             'Z2':Z2,\n",
        "             'A2':A2\n",
        "            }\n",
        "    # print(A2.shape)\n",
        "    return A2, cache"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPCi62-NGx09"
      },
      "source": [
        "def backward_pass(parameters, cache, X, Y):\n",
        "    \n",
        "    # unpack paramaeters and cache to get values for backpropagation calculations\n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "    \n",
        "    Z1 = cache['Z1']\n",
        "    A1 = cache['A1']\n",
        "    Z2 = cache['Z2']\n",
        "    A2 = cache['A2']\n",
        "    \n",
        "    m = X.shape[1] # number of examples in a training set\n",
        "    #print(A2)\n",
        "    #print(A2.shape)\n",
        "    dZ2= A2 - Y\n",
        "    \n",
        "    \n",
        "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)   # keepdims - prevents python to output rank 1 array (n,)\n",
        "    \n",
        "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2)) # we use tanh activation function\n",
        "    dW1 = (1 / m) * np.dot(dZ1, X.T)  \n",
        "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "    \n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "    \n",
        "    return grads"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvgLoY9jGx7l"
      },
      "source": [
        "def initialize_adam(parameters) :\n",
        "       \n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    s = {}\n",
        "    \n",
        "    # v- exponentially weighted average of the gradient\n",
        "    # s -exponentially weighted average of the squared gradient\n",
        "    for l in range(L):\n",
        "        v[\"dW1\" ] = np.zeros(parameters[\"W1\" ].shape)\n",
        "        v[\"db1\" ] = np.zeros(parameters[\"b1\" ].shape)\n",
        "        s[\"dW2\" ] = np.zeros(parameters[\"W2\" ].shape)\n",
        "        s[\"db2\" ] = np.zeros(parameters[\"b2\" ].shape)\n",
        "        \n",
        "    return v, s"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn3eHQeZ9Fze"
      },
      "source": [
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
        "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
        "   \n",
        "  \n",
        "    \n",
        "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
        "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
        "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
        "    \n",
        "        \n",
        "    for l in range(L):\n",
        "        v[\"dW1\"] = beta1*v[\"dW1\" ] + (1-beta1)*grads['dW1' ] \n",
        "        v[\"db1\" ] = beta1*v[\"db1\" ] + (1-beta1)*grads['db1' ]\n",
        "       \n",
        "\n",
        "        # Compute bias-corrected first moment estimate. \n",
        "       \n",
        "        v_corrected[\"dW1\" ] = v[\"dW1\" ]/(1 - beta1**t)\n",
        "        v_corrected[\"db1\" ] = v[\"db1\" ]/(1 - beta1**t)\n",
        "\n",
        "\n",
        "        # Moving average of the squared gradients.\n",
        "     \n",
        "        s[\"dW2\"] = beta2*s[\"dW2\" ] + (1-beta2)*(grads['dW2' ])**2\n",
        "        s[\"db2\" ] = beta2*s[\"db2\" ] + (1-beta2)*(grads['db2' ])**2\n",
        "       \n",
        "\n",
        "        # Compute bias-corrected second raw moment estimate. \n",
        "       \n",
        "        s_corrected[\"dW2\" ] = s[\"dW2\" ]/(1 - beta2**t)\n",
        "        s_corrected[\"db2\" ] = s[\"db2\" ]/(1 - beta2**t)\n",
        "        \n",
        "\n",
        "        parameters[\"W1\" ] = parameters[\"W1\" ]- (learning_rate*v_corrected[\"dW1\" ])/(np.sqrt(s_corrected[\"dW2\" ].T+epsilon))\n",
        "        parameters[\"b1\" ] = parameters[\"b1\" ]- (learning_rate*v_corrected[\"db1\" ])/(np.sqrt(s_corrected[\"db2\" ].T+epsilon))\n",
        "   \n",
        "    return parameters, v, s"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZsllRa_9F11"
      },
      "source": [
        "def NN_model(X,Y,n_h, num_iterations, learning_rate):\n",
        "    \n",
        "    n_x = X.shape[0] # size of an input layer = number of features \n",
        "    n_y = Y.shape[0] # size of an output layer\n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "    \n",
        "    #unpack parameters\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    #print(parameters)\n",
        "    param = {}\n",
        "    for i in range(num_iterations):\n",
        "      #print(i)  \n",
        "      A2, cache = forward_pass(X, parameters)\n",
        "      grads = backward_pass(parameters, cache, X, Y)\n",
        "      v,s = initialize_adam(parameters)\n",
        "      param = update_parameters_with_adam(parameters,grads,v,s , t = 2 )\n",
        "      print(i,param)\n",
        "    #print(A2)\n",
        "    # print(cache)    \n",
        "    return parameters"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bD3yr1R9F3i"
      },
      "source": [
        "def predict(parameters, X):\n",
        "    \n",
        "    A2, cache = forward_pass(X, parameters)\n",
        "    predictions = np.round(A2)\n",
        "    \n",
        "    return predictions"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86lTpXnE9F6i"
      },
      "source": [
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znCw5UeF9F9Z"
      },
      "source": [
        "def loadbatch(batchname):\n",
        "    batch = unpickle('/content/drive/MyDrive/Colab_Notebooks'+\"/\"+batchname)\n",
        "    return batch"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z9EcVlW9F_o"
      },
      "source": [
        "def loadlabelnames():\n",
        "    meta = unpickle('/content/drive/MyDrive/Colab_Notebooks'+\"/\"+'batches.meta')\n",
        "    return meta[b'label_names']"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sq41GrhL9GCQ",
        "outputId": "eb454596-1fd2-42a9-fb37-ddcff15c80fd"
      },
      "source": [
        "batch1 = loadbatch('data_batch_1')\n",
        "print(\"Number of items in the batch is\", len(batch1))\n",
        "\n",
        "# Display all keys, so we can see the ones we want\n",
        "print('All keys in the batch:', batch1.keys())"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of items in the batch is 4\n",
            "All keys in the batch: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Y8jtq3lCxFRL",
        "outputId": "3d87e74b-b31a-4a49-a4c6-9380bab6e060"
      },
      "source": [
        "data = batch1[b'data'][:,:1023]\n",
        "labels = batch1[b'labels']\n",
        "print (\"size of data in this batch:\", len(data), \", size of labels:\", len(labels))\n",
        "print (type(data))\n",
        "# print(data.shape)\n",
        "# print(labels)\n",
        "label = []\n",
        "data_class = []\n",
        "names = loadlabelnames()"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of data in this batch: 10000 , size of labels: 10000\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOBNutyK9GGv"
      },
      "source": [
        "for i in range(len(labels)):\n",
        "  if labels[i] == 1 or labels[i]==6:\n",
        "    label.append(labels[i])\n",
        "    data_class.append(data[i])\n",
        "\n"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zrzFJOv9GKk"
      },
      "source": [
        "df = np.array(data_class)\n",
        "df2 = np.array(label)\n",
        "\n",
        "\n",
        "num_train = int(.70 * len(df))\n",
        "num_test = int(0.15 * len(df))\n",
        "\n",
        "x_train, y_train = data_class[:num_train], label[:num_train]\n",
        "x_test, y_test = data_class[num_test:], label[num_test:]\n",
        "\n",
        "x_train  = np.array(x_train)\n",
        "y_train  = np.array(y_train)\n",
        "x_test  = np.array(x_test)\n",
        "y_test  = np.array(y_test)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kPlai2m9GNV"
      },
      "source": [
        "arr = [y_train.tolist()]\n",
        "y_train = np.array(arr)\n",
        "\n",
        "arr1 = [y_test.tolist()]\n",
        "y_test = np.array(arr1)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-9_yIhY9GPK"
      },
      "source": [
        "num_iterations = 20000\n",
        "learning_rate = 0.01\n",
        "n_h = 4\n",
        "parameters_final = NN_model(x_train.T,y_train,n_h, num_iterations, learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VQAw4DAn9vRw",
        "outputId": "c673a032-3a0f-455b-c58c-6d66653fa051"
      },
      "source": [
        "Y_predictions_test = predict(parameters_final, x_test.T)\n",
        "Y_predictions_train = predict(parameters_final, x_train.T)\n",
        "\n",
        "\n",
        "acc = np.mean(y_train == Y_predictions_train)\n",
        "acc"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.47360912981455067"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVhkQynQSO4H"
      },
      "source": [
        "### Resources - \n",
        "1. https://towardsdatascience.com/how-to-implement-an-adam-optimizer-from-scratch-76e7b217f1cc\n",
        "2. https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c\n",
        "3. https://machinelearningmastery.com/tour-of-optimization-algorithms/\n",
        "                \n",
        "\n"
      ]
    }
  ]
}