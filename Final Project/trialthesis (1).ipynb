{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trialthesis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.3 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.3"
    },
    "accelerator": "TPU",
    "interpreter": {
      "hash": "00c26e0d0bc6092809178b08f74d162f3ce833ae45d469755623bafa78f7cfc2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciRiOdOV8jti"
      },
      "source": [
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from __future__ import print_function, division\n",
        "from keras.layers import Input, Dense, Reshape, Dropout,Bidirectional,LSTM,Embedding\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Check that we are using a GPU, if not switch runtimes\n",
        "#   using Runtime > Change Runtime Type > GPU\n",
        "#assert len(tf.config.list_physical_devices('GPU')) > 0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'CuDNNLSTM' from 'keras.layers' (C:\\Users\\CS-Guest-2\\anaconda3\\lib\\site-packages\\keras\\layers\\__init__.py)",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-2-2a5b8b56ae3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCuDNNLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZeroPadding2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvanced_activations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'CuDNNLSTM' from 'keras.layers' (C:\\Users\\CS-Guest-2\\anaconda3\\lib\\site-packages\\keras\\layers\\__init__.py)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ene-WKVH9tI4"
      },
      "source": [
        "\n",
        "import csv\n",
        "music_data = []\n",
        "with open(\"tunes.csv\", \"r\") as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    for lines in csv_reader:\n",
        "      music_data.append(lines[1])\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnec0tUD8ny9"
      },
      "source": [
        "joinedsongs = []\n",
        "for i in music_data:\n",
        "  tune = \"\\n\".join(i)\n",
        "  joinedsongs.append(tune)\n",
        "  \n",
        "final_tunes = \"\\n\\n\".join(joinedsongs) "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu1hx-xF9BOJ",
        "outputId": "67c57490-5898-4eba-e15a-7f9a49242f57"
      },
      "source": [
        "# Find all unique characters in the joined string\n",
        "vocab = sorted(set(final_tunes))\n",
        "print(\"There are\", len(vocab), \"unique characters in the dataset\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 122 unique characters in the dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fLfQVOE9I_3"
      },
      "source": [
        "def prepare_sequences(notes, n_vocab):\n",
        "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
        "    vocab = sorted(set(notes))\n",
        "    sequence_length = len(vocab)\n",
        "\n",
        "    \n",
        "    char2int = {u:i for i, u in enumerate(vocab)}\n",
        "    network_input = []\n",
        "    network_output = []\n",
        "\n",
        "    # create input sequences and the corresponding outputs\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i:i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length]\n",
        "        network_input.append([char2int[char] for char in sequence_in])\n",
        "        network_output.append(char2int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "    # Reshape the input into a format compatible with LSTM layers\n",
        "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "\n",
        "\n",
        "    return (network_input, network_output)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpqNldljRlUl"
      },
      "source": [
        "x,y = prepare_sequences(final_tunes,len(set(final_tunes)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppu1Ur4c_it-",
        "outputId": "936d0c67-c114-4658-94ab-7e94a763b2f1"
      },
      "source": [
        "class GAN():\n",
        "    def __init__(self, rows):\n",
        "        self.seq_length = rows\n",
        "        self.seq_shape = (self.seq_length, 1)\n",
        "        self.latent_dim = 100\n",
        "        self.disc_loss = []\n",
        "        self.gen_loss =[]\n",
        "        \n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "        \n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generates note sequences\n",
        "        z = Input(shape=(1,self.latent_dim))\n",
        "        generated_seq = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        validity = self.discriminator(generated_seq)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, validity)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def lstm_gen(rnn_units): \n",
        "        return tf.keras.layers.LSTM(\n",
        "            rnn_units, \n",
        "            return_sequences=True, \n",
        "            recurrent_initializer='glorot_uniform',\n",
        "            recurrent_activation='sigmoid',\n",
        "            stateful=True, \n",
        "            )\n",
        "    \n",
        "    def build_discriminator(self):\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(512,return_sequences=True,input_shape = self.seq_shape))\n",
        "        model.add(Bidirectional(LSTM(512)))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(256))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.summary()\n",
        "        seq = Input(shape=self.seq_shape)\n",
        "        validity = model(seq)\n",
        "\n",
        "        return Model(seq, validity)                \n",
        "\n",
        "    def build_generator(self):\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(256,input_shape = (1,self.latent_dim),return_sequences=True))\n",
        "        model.add(Dropout(0.6))\n",
        "        model.add(LSTM(128,input_shape = (1,self.latent_dim),return_sequences=True))\n",
        "        model.add(Dropout(0.6))\n",
        "        model.add(LSTM(64,input_shape = (1,self.latent_dim),return_sequences=False))\n",
        "        model.add(Dropout(0.6))\n",
        "        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n",
        "        model.add(Reshape(self.seq_shape))\n",
        "        model.summary()\n",
        "        \n",
        "        noise = Input(shape=(1,self.latent_dim))\n",
        "        seq = model(noise)\n",
        "\n",
        "        return Model(noise, seq)\n",
        "\n",
        "    def train(self,final_tunes, epochs,gen_len, batch_size=32, sample_interval=50):\n",
        "\n",
        "        # Load and convert the data\n",
        "        notes = final_tunes\n",
        "        n_vocab = len(set(final_tunes))\n",
        "        X_train, y_train = prepare_sequences(notes, n_vocab)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        real = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "        \n",
        "        # Training the model\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # Training the discriminator\n",
        "            # Select a random batch of note sequences\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            real_seqs = X_train[idx]\n",
        "\n",
        "            #noise = np.random.choice(range(484), (batch_size, self.latent_dim))\n",
        "            #noise = (noise-242)/242\n",
        "            noise = np.random.normal(0, 1, (batch_size, 1,self.latent_dim))\n",
        "            gen_seqs = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "\n",
        "            #  Training the Generator\n",
        "            noise = np.random.normal(0, 1, (batch_size, 1,self.latent_dim))\n",
        "            # Train the generator (to have the discriminator label samples as real)\n",
        "            g_loss = self.combined.train_on_batch(noise, real)\n",
        "\n",
        "            # Print the progress and save into loss lists\n",
        "            if epoch % sample_interval == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "              self.disc_loss.append(d_loss[0])\n",
        "              self.gen_loss.append(g_loss)\n",
        "        \n",
        "        op = self.generate(notes,gen_len)\n",
        "        self.plot_loss()\n",
        "        return op\n",
        "\n",
        "    def generate(self, input_notes,gen_len):\n",
        "        # Get pitch names and store in a dictionary\n",
        "        notes = input_notes\n",
        "        final = []\n",
        "        pitchnames = sorted(set(item for item in notes))\n",
        "        idx2char = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "        # Use random noise to generate sequences\n",
        "        for i in range(gen_len):\n",
        "          noise = np.random.normal(0, 1, (1,1, self.latent_dim))\n",
        "          predictions = self.generator.predict(noise)\n",
        "          #print('pred:',predictions)\n",
        "          predictions = tf.squeeze(predictions, 0)\n",
        "          predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "          print('id:',predicted_id)\n",
        "          #final.append(idx2char[predicted_id])\n",
        "\n",
        "\n",
        "    def plot_loss(self):\n",
        "        plt.plot(self.disc_loss, c='red')\n",
        "        plt.plot(self.gen_loss, c='blue')\n",
        "        plt.title(\"GAN Loss per Epoch\")\n",
        "        plt.legend(['Discriminator', 'Generator'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.savefig('GAN_Loss_per_Epoch_final.png', transparent=True)\n",
        "        plt.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  gan = GAN(rows=len(vocab))    \n",
        "  op = gan.train(final_tunes,epochs=1000,gen_len=20, batch_size=32, sample_interval=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 122, 512)          1052672   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 1024)              4198400   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 5,907,457\n",
            "Trainable params: 5,907,457\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_2 (LSTM)                (None, 1, 256)            365568    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 1, 128)            197120    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1, 128)            0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 64)                49408     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 122)               7930      \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 122, 1)            0         \n",
            "=================================================================\n",
            "Total params: 620,026\n",
            "Trainable params: 620,026\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsMwzjPXYa6d"
      },
      "source": [
        "predictions = tf.squeeze(op, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj3koHIXYidB"
      },
      "source": [
        "predicted_id = tf.random.categorical(op[0], num_samples=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}